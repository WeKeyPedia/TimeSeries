{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named wekeypedia.wikipedia.page",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0766c6924e11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwekeypedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWikipediaPage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mPage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named wekeypedia.wikipedia.page"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats=['svg']\n",
    "\n",
    "import json\n",
    "import codecs\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from wekeypedia.wikipedia.page import WikipediaPage as Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gather_pages_data(dataset_dir_name, list_of_page_names):\n",
    "    data_pages_dir_name = '%s/data/pages/' % dataset_dir_name\n",
    "    if not(os.path.exists(data_pages_dir_name)): os.makedirs(data_pages_dir_name)\n",
    "\n",
    "    pages_data = {}\n",
    "    for page_name in list_of_page_names:\n",
    "        file_name = '%s/%s.json'%(data_pages_dir_name, page_name)\n",
    "        if (os.path.exists(file_name)):\n",
    "            with open(file_name) as f:\n",
    "                pages_data[page_name] = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "            wikipage = Page(title=page_name)\n",
    "            request = wikipage.fetch_info(page_name)['query']['pages']\n",
    "            page_id = list(request)[0]\n",
    "            if page_id!='-1':\n",
    "                try:\n",
    "                    for x in request[page_id]:\n",
    "                        data[x]=request[page_id][x]\n",
    "                    data['revisions']=wikipage.get_revisions_list()\n",
    "                    data['links']= wikipage.get_links()\n",
    "                    data['categories']= wikipage.get_categories()\n",
    "                    pages_data[page_name]=data\n",
    "                    f = open(file_name,'w')\n",
    "                    f.write(json.dumps(data))\n",
    "                    f.close()\n",
    "                except Exception as e:\n",
    "                    print 'Error with page:',page_name\n",
    "                    print e\n",
    "    return(pages_data)\n",
    "\n",
    "\n",
    "def gather_pages_views(dataset_dir_name,list_of_page_names):\n",
    "    pages_views_dir_name = '%s/data/pagesviews/' % dataset_dir_name\n",
    "    if not(os.path.exists(pages_views_dir_name)): os.makedirs(pages_views_dir_name)\n",
    "\n",
    "    error = []\n",
    "    for page_name in list_of_page_names:\n",
    "        file_name = '%s/%s.json' % (pages_views_dir_name,page_name)\n",
    "        if not (os.path.exists(file_name)):            \n",
    "            try:\n",
    "                wikipage = Page(title=page_name)\n",
    "                page_views_ts = {k:v for x in wikipage.get_pageviews() for (k,v) in x.items()}\n",
    "                f = open(filename,'w')\n",
    "                f.write(json.dumps(page_views_ts))\n",
    "                f.close()\n",
    "            except:\n",
    "                error.append(page_name)\n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Compute pages views time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pages_views_time_series(dataset_dir_name,list_of_page_names):\n",
    "    pages_views_dir_name = '%s/data/pagesviews/' % dataset_dir_name\n",
    "\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    if not(os.path.exists(time_series_dir_name)): os.makedirs(time_series_dir_name)    \n",
    "\n",
    "    pages_views_daily_ts={}\n",
    "    pages_views_weekly_ts={}       \n",
    "    pages_views_monthly_ts={}\n",
    "    pages_views_yearly_ts={}\n",
    "    \n",
    "    for page_name in list_of_page_names:\n",
    "        file_name = '%s/%s.json'%(pages_views_dir_name,page_name)\n",
    "        file_name_daily = '%s/%s.pageviews.daily.csv' % (time_series_dir_name,page_name)\n",
    "        file_name_weekly = '%s/%s.pageviews.weekly.csv' % (time_series_dir_name,page_name)\n",
    "        file_name_monthly = '%s/%s.pageviews.monthly.csv' % (time_series_dir_name,page_name)\n",
    "        file_name_yearly = '%s/%s.pageviews.yearly.csv' % (time_series_dir_name,page_name)\n",
    "    \n",
    "        if os.path.exists(file_name):\n",
    "            if (os.path.exists(file_name_daily)):\n",
    "                pages_views_daily_ts[page_name] = pd.DataFrame().from_csv(file_name_daily)\n",
    "                pages_views_weekly_ts[page_name] = pd.DataFrame().from_csv(file_name_weekly)\n",
    "                pages_views_monthly_ts[page_name] = pd.DataFrame().from_csv(file_name_monthly)\n",
    "                pages_views_yearly_ts[page_name] = pd.DataFrame().from_csv(file_name_yearly)\n",
    "            else:\n",
    "                data=json.load(open(file_name)).items()\n",
    "                index = []\n",
    "                series = []\n",
    "                for k,v in data:\n",
    "                    try:\n",
    "                        index.append(pd.to_datetime(k, format=\"%Y-%m-%d\"))\n",
    "                        series.append(v)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                df = pd.DataFrame(series,index=index,columns=['page_views'])\n",
    "                pages_views_daily_ts[page_name]=df\n",
    "                pages_views_daily_ts[page_name].to_csv(file_name_daily,encoding=\"utf-8\")\n",
    "                pages_views_weekly_ts[page_name] = df.resample('W-MON', how='sum')\n",
    "                pages_views_weekly_ts[page_name].to_csv(file_name_weekly,encoding=\"utf-8\")\n",
    "                pages_views_monthly_ts[page_name] = df.resample('M', how='sum')\n",
    "                pages_views_monthly_ts[page_name].to_csv(file_name_monthly,encoding=\"utf-8\")\n",
    "                pages_views_yearly_ts[page_name] = df.resample('A', how='sum')\n",
    "                pages_views_yearly_ts[page_name].to_csv(file_name_yearly,encoding=\"utf-8\")\n",
    "    return(pages_views_daily_ts,pages_views_weekly_ts,pages_views_monthly_ts,pages_views_yearly_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Revisions time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pages_revisions_time_series_gen(pages_revisions,suffixe,time_series_dir_name):\n",
    "    \n",
    "    revisions_daily_ts={}\n",
    "    revisions_weekly_ts={}       \n",
    "    revisions_monthly_ts={}\n",
    "    revisions_yearly_ts={}   \n",
    "    for page_name in pages_revisions.keys():\n",
    "        file_name_daily = '%s/%s.%s.daily.csv'%(time_series_dir_name,suffixe,page_name)\n",
    "        file_name_weekly = '%s/%s.%s.weekly.csv'%(time_series_dir_name,suffixe,page_name)\n",
    "        file_name_monthly = '%s/%s.%s.monthly.csv'%(time_series_dir_name,suffixe,page_name)\n",
    "        file_name_yearly = '%s/%s.%s.yearly.csv'%(time_series_dir_name,suffixe,page_name)\n",
    "        \n",
    "        if (os.path.exists(file_name_daily)):\n",
    "            revisions_daily_ts[page_name] = pd.DataFrame().from_csv(file_name_daily)\n",
    "            revisions_weekly_ts[page_name] = pd.DataFrame().from_csv(file_name_weekly)\n",
    "            revisions_monthly_ts[page_name] = pd.DataFrame().from_csv(file_name_monthly)\n",
    "            revisions_yearly_ts[page_name] = pd.DataFrame().from_csv(file_name_yearly)\n",
    "        else:\n",
    "            revisions=pages_revisions[page_name]\n",
    "            \n",
    "            if len(revisions)==0:\n",
    "                #print 'No revisions for %s with suffixe: %s' % (page_name,suffixe)\n",
    "                continue\n",
    "            \n",
    "            df = pd.DataFrame(revisions)\n",
    "            df[\"datetime\"] = df.timestamp.apply(pd.to_datetime)\n",
    "            df[\"day\"] = df.datetime.apply(dt.date.strftime, args=('%Y-%m-%d',))\n",
    "            counts = df.groupby(df[\"day\"])\n",
    "            counts = counts.aggregate(len)\n",
    "            series = counts[\"size\"].tolist()\n",
    "            index = counts.index.map(lambda x: pd.to_datetime(x, format=\"%Y-%m-%d\"))\n",
    "            df = pd.DataFrame(series,columns=['revisions'],index=index)\n",
    "            \n",
    "            revisions_daily_ts[page_name]=df\n",
    "            revisions_daily_ts[page_name]=revisions_daily_ts[page_name].fillna(0)\n",
    "            revisions_daily_ts[page_name].to_csv(file_name_daily,encoding=\"utf-8\")\n",
    "            \n",
    "            revisions_weekly_ts[page_name]=df.resample('W-MON', how='sum')\n",
    "            revisions_weekly_ts[page_name]=revisions_weekly_ts[page_name].fillna(0)           \n",
    "            revisions_weekly_ts[page_name].to_csv(file_name_weekly,encoding=\"utf-8\")\n",
    "\n",
    "            revisions_monthly_ts[page_name]=df.resample('M', how='sum')\n",
    "            revisions_monthly_ts[page_name]=revisions_monthly_ts[page_name].fillna(0)\n",
    "            revisions_monthly_ts[page_name].to_csv(file_name_monthly,encoding=\"utf-8\")\n",
    "            \n",
    "            revisions_yearly_ts[page_name]=df.resample('A', how='sum')\n",
    "            revisions_yearly_ts[page_name]=revisions_yearly_ts[page_name].fillna(0)\n",
    "            revisions_yearly_ts[page_name].to_csv(file_name_yearly,encoding=\"utf-8\")\n",
    "    \n",
    "    return(revisions_daily_ts,revisions_weekly_ts,revisions_monthly_ts,revisions_yearly_ts)\n",
    "\n",
    "\n",
    "## ## ## ## ## ## ## ## ## ##\n",
    "\n",
    "\n",
    "def get_pages_revisions_time_series(dataset_dir_name,list_of_page_names):\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    if not(os.path.exists(time_series_dir_name)): os.makedirs(time_series_dir_name)    \n",
    "\n",
    "    pages_data = gather_pages_data(dataset_dir_name,list_of_page_names)\n",
    "    \n",
    "    data = {}\n",
    "    for k,v in pages_data.iteritems():\n",
    "            data[k]=v['revisions']\n",
    "    \n",
    "    return get_pages_revisions_time_series_gen(data,'revisions',time_series_dir_name)\n",
    "\n",
    "def get_pages_ip_revisions_time_series(dataset_dir_name,list_of_page_names):\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    if not(os.path.exists(time_series_dir_name)): os.makedirs(time_series_dir_name)    \n",
    "   \n",
    "    pages_data = gather_pages_data(dataset_dir_name,list_of_page_names)\n",
    "\n",
    "    data = {}\n",
    "    for k,v in pages_data.iteritems():\n",
    "            data[k]=[x for x in v['revisions'] if ('userid' in x and x['userid']==0)]\n",
    "    return get_pages_revisions_time_series_gen(data,'revisions.ip',time_series_dir_name)\n",
    "\n",
    "def get_pages_bot_revisions_time_series(dataset_dir_name,list_of_page_names):\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    if not(os.path.exists(time_series_dir_name)): os.makedirs(time_series_dir_name)    \n",
    "\n",
    "    pages_data = gather_pages_data(dataset_dir_name,list_of_page_names)\n",
    "\n",
    "    data = {}\n",
    "    for k,v in pages_data.iteritems():\n",
    "        data[k]=[x for x in v['revisions'] if ('user' in x and 'bot' in x['user'].lower())]\n",
    "    \n",
    "    return get_pages_revisions_time_series_gen(data,'revisions.bot',time_series_dir_name)\n",
    "\n",
    "def get_pages_members_revisions_time_series(dataset_dir_name,list_of_page_names):\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    if not(os.path.exists(time_series_dir_name)): os.makedirs(time_series_dir_name)    \n",
    "\n",
    "    pages_data = gather_pages_data(dataset_dir_name,list_of_page_names)\n",
    "\n",
    "    data = {}\n",
    "    for k,v in pages_data.iteritems():\n",
    "        data[k]=[x for x in v['revisions'] if ('user' in x and x['userid']!=0 and 'bot' not in x['user'].lower())]\n",
    "    \n",
    "    return get_pages_revisions_time_series_gen(data,'revisions.members',time_series_dir_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Gather and compute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gather_and_compute_data(dataset_dir_name,list_of_page_names):\n",
    "    gather_pages_data(dataset_dir_name, list_of_page_names)\n",
    "    gather_pages_views(dataset_dir_name,list_of_page_names)\n",
    "    get_pages_views_time_series(dataset_dir_name,list_of_page_names)\n",
    "    get_pages_members_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "    get_pages_bot_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "    get_pages_ip_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "    get_pages_revisions_time_series(dataset_dir_name,list_of_page_names) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Statistics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stat_computation(dataset_dir_name,list_of_page_names):\n",
    "    pages_data=gather_pages_data(dataset_dir_name, list_of_page_names)\n",
    "    df = pd.DataFrame(index=pages_data.keys())\n",
    "    \n",
    "    #pageid\n",
    "    data={k:v['pageid'] for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Page_id'] = data[k]\n",
    "    #length\n",
    "    data={k:v['length'] for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Length'] = data[k]\n",
    "    #namespace\n",
    "    data={k:v['ns'] for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Namespace'] = data[k]\n",
    "    #nombre de revisions\n",
    "    data={k:len(v['revisions']) for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Nb_revisions'] = data[k]\n",
    "    #nombre de revisions by IP\n",
    "    data={k:len([x for x in v['revisions'] if ('userid' in x and x['userid']==0)])\n",
    "            for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Nb_revisions_IP'] = data[k]\n",
    "    #nombre de revisions by Bot\n",
    "    data={k:len([x for x in v['revisions'] if ('user' in x and 'bot' in x['user'].lower())])\n",
    "            for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Nb_revisions_Bot'] = data[k]\n",
    "    #nombre de revisions by Alive Registered Members\n",
    "    data={k:len([x for x in v['revisions'] if ('user' in x and x['userid']!=0 and 'bot' not in x['user'].lower())])\n",
    "            for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Nb_revisions_wiki'] = data[k]\n",
    "    #nombre de contributeurs\n",
    "    data={k:len(set([x['user'] for x in v['revisions'] if 'user' in x]))\n",
    "            for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Nb_editors'] = data[k]\n",
    "    #nombre de contributeurs IP\n",
    "    data={k:len(set([x['user'] for x in v['revisions'] if 'userid' in x and x['userid']==0]))\n",
    "            for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Nb_editors_IP'] = data[k]\n",
    "    #nombre de contributeurs Bot\n",
    "    data={k:len(set([x['user'] for x in v['revisions'] if 'user' in x and 'bot' in x['user']]))\n",
    "            for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Nb_editors_Bot'] = data[k]\n",
    "    #nombre de contributeurs by Alive Registered Members\n",
    "    data={k:len(set([x['user'] for x in v['revisions'] if 'user' in x and x['userid']!=0 and 'bot' not in x['user']]))\n",
    "            for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Nb_editors_wiki'] = data[k]\n",
    "    #nombre de revisions\n",
    "    data={k:len(v['links']) for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Links'] = data[k]\n",
    "    #date of the first contibutions (in number of days after the start of the wikipedia project)\n",
    "    def numberOfDaysAfter(date):\n",
    "        return( (datetime.datetime.strptime(date,\"%Y-%m-%dT%H:%M:%SZ\")-datetime.datetime.strptime(\"2001-01-15T00:00:00Z\",\"%Y-%m-%dT%H:%M:%SZ\")).days)\n",
    "    data={k:min(map(numberOfDaysAfter,map(lambda x: x['timestamp'],v['revisions'])))  for k,v in pages_data.items()}\n",
    "    for k in pages_data.keys(): df.ix[k,'Date'] = data[k]\n",
    "    #number of pageviews in 2011\n",
    "    a,b,c,data = get_pages_views_time_series(dataset_dir_name,list_of_page_names)\n",
    "    for k in pages_data.keys(): df.ix[k,'Page_views_2010'] = data[k]['page_views']['2010-12-31']\n",
    "\n",
    "    #quality\n",
    "    def get_quality(page):\n",
    "        talk_name = 'Talk:'+page\n",
    "        talk = gather_pages_data(dataset_dir_name, [ talk_name ])\n",
    "        if len(talk)==0: return 0,0\n",
    "        quality=0\n",
    "        islist=0\n",
    "        for c in talk[talk_name]['categories']:\n",
    "            cat = c['title'].lower()\n",
    "            if '-class' in cat:\n",
    "                if 'fa-class' in cat or 'fl-class' in cat: quality = max(quality,7)\n",
    "                if 'a-class' in cat and 'ga-class' not in cat : quality = max(quality,6)\n",
    "                if 'ga-class' in cat: quality = max(quality,5)\n",
    "                if 'b-class' in cat and 'stub-class' not in cat: quality = max(quality,4)\n",
    "                if 'c-class' in cat: quality = max(quality,3)\n",
    "                if 'start-class' in cat: quality = max(quality,2)\n",
    "                if 'stub-class' in cat: quality = max(quality,1)\n",
    "                if 'fl-class' in cat or 'list-class' in cat: islist=1\n",
    "        return quality,islist\n",
    "    \n",
    "    for k in pages_data.keys(): \n",
    "        quality,islist=get_quality(k)\n",
    "        df.ix[k,'Quality'] = quality\n",
    "        df.ix[k,'Is_list'] = islist\n",
    "\n",
    "        \n",
    "    return(df)\n",
    "\n",
    "\n",
    "# yearly time series revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'gather_pages_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-574cf7861ae7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgetquality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Pi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-574cf7861ae7>\u001b[0m in \u001b[0;36mgetquality\u001b[1;34m(page)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetquality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m         \u001b[0mtalk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgather_pages_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_dir_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Talk:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtalk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mcategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtalk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'categories'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'gather_pages_data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Report for evolution of revisions and pageviews for a set of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def compute_sum_ts(dic_ts,col_name):\n",
    "    df = pd.DataFrame()\n",
    "    for title in dic_ts:\n",
    "        if (len(df.columns)==0):\n",
    "            df = dic_ts[title].copy()\n",
    "        else:\n",
    "            tmp = dic_ts[title].copy()\n",
    "            tmp.columns = [x.replace(col_name,'tmp') for x in tmp.columns]\n",
    "            df = df.join(tmp,how='outer').fillna(0)\n",
    "            df['sum'] = df[col_name] + df['tmp']\n",
    "            df.drop(col_name, axis=1, inplace=True)\n",
    "            df.drop('tmp', axis=1, inplace=True)\n",
    "            df.columns = [col_name]\n",
    "    return(df)\n",
    "\n",
    "        \n",
    "        \n",
    "def get_report_set_of_pages(revisions_ip_ts,revisions_bot_ts,revisions_members_ts,pageviews_ts,ip=True,bot=True,member=True):\n",
    "    display(HTML(\"<h2>Evolution of pageviews and revisions</h2>\" ))\n",
    "   \n",
    "    df_pageviews = compute_sum_ts(pageviews_ts,'page_views')\n",
    "\n",
    "    df_ip = compute_sum_ts(revisions_ip_ts,'revisions_ip').join(df_pageviews,how='outer').fillna(0)  \n",
    "    df_bot = compute_sum_ts(revisions_bot_ts,'revisions_bot').join(df_pageviews,how='outer').fillna(0)\n",
    "    df_members = compute_sum_ts(revisions_members_ts,'revisions_members').join(df_pageviews,how='outer').fillna(0)\n",
    "\n",
    "    if ip:\n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by ip</h3>\" ))\n",
    "\n",
    "        df_ip['revisions_ip'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_ip['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n",
    "        \n",
    "    if member:      \n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by members</h3>\" ))\n",
    "\n",
    "        df_members['revisions_members'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_members['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n",
    "\n",
    "    if bot:\n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by bot</h2>\" ))\n",
    "\n",
    "        df_bot['revisions_bot'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_bot['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "def get_monthly_report_set_of_pages(dataset_dir_name,list_of_page_names,ip=True,bot=True,member=True):\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "\n",
    "    a,b,revisions_ip_ts,c = get_pages_ip_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "    a,b,revisions_bot_ts,c = get_pages_bot_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "    a,b,revisions_members_ts,c = get_pages_members_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "\n",
    "    for title in list_of_page_names:\n",
    "        try:\n",
    "            revisions_ip_ts[title].columns = [x.replace('revisions','revisions_ip') for x in revisions_ip_ts[title].columns]\n",
    "        except :\n",
    "            print 'No revisions ts found for ip for page',title\n",
    "        try:\n",
    "            revisions_bot_ts[title].columns = [x.replace('revisions','revisions_bot') for x in revisions_bot_ts[title].columns]\n",
    "        except :\n",
    "            print 'No revisions ts found for bot for page',title\n",
    "        try:\n",
    "            revisions_members_ts[title].columns = [x.replace('revisions','revisions_members') for x in revisions_members_ts[title].columns]\n",
    "        except :\n",
    "            print 'No revisions ts found for members for page',title\n",
    "\n",
    "    pages_views_dir_name = '%s/data/pagesviews/' % dataset_dir_name\n",
    "    a,b,pages_views_ts,c = get_pages_views_time_series(dataset_dir_name,list_of_page_names)\n",
    "\n",
    "   \n",
    "    get_report_set_of_pages(revisions_ip_ts,revisions_bot_ts,revisions_members_ts,pages_views_ts,ip=ip,bot=bot,member=member)\n",
    "    \n",
    "    \n",
    "def get_yearly_report_set_of_pages(dataset_dir_name,list_of_page_names,ip=True,bot=True,member=True):\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "\n",
    "    a,b,c,revisions_ip_ts = get_pages_ip_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "    a,b,c,revisions_bot_ts = get_pages_bot_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "    a,b,c,revisions_members_ts = get_pages_members_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "\n",
    "    for title in list_of_page_names:\n",
    "        try:\n",
    "            revisions_ip_ts[title].columns = [x.replace('revisions','revisions_ip') for x in revisions_ip_ts[title].columns]\n",
    "        except :\n",
    "            print 'No revisions ts found for ip for page',title\n",
    "        try:\n",
    "            revisions_bot_ts[title].columns = [x.replace('revisions','revisions_bot') for x in revisions_bot_ts[title].columns]\n",
    "        except :\n",
    "            print 'No revisions ts found for bot for page',title\n",
    "        try:\n",
    "            revisions_members_ts[title].columns = [x.replace('revisions','revisions_members') for x in revisions_members_ts[title].columns]\n",
    "        except :\n",
    "            print 'No revisions ts found for members for page',title\n",
    "\n",
    "    pages_views_dir_name = '%s/data/pagesviews/' % dataset_dir_name\n",
    "    a,b,c,pages_views_ts = get_pages_views_time_series(dataset_dir_name,list_of_page_names)\n",
    "\n",
    "   \n",
    "    get_report_set_of_pages(revisions_ip_ts,revisions_bot_ts,revisions_members_ts,pages_views_ts,ip=ip,bot=bot,member=member)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
